{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take-home exam3의 내용\n",
    "\n",
    "GNI 코퍼스로 부터 약어와 특수용어들을 가져온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import *\n",
    "from nltk import *\n",
    "corpus_root = 'C:/Users/Maeg/nltk_data/Genomics-Informatics-Corpus-master/Genomics-Informatics-Corpus-master/raw_text2'\n",
    "GNIcorpus = PlaintextCorpusReader(corpus_root,'.*\\.txt',encoding='utf-16')\n",
    "giRaw = GNIcorpus.raw('gni-4-1-23.txt')\n",
    "giRaw10 = GNIcorpus.raw()\n",
    "giword_list = nltk.word_tokenize(giRaw10)\n",
    "giword = GNIcorpus.words()\n",
    "fileid = GNIcorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리\n",
    "giRaw1 = re.sub(r'\\ufeff|\\x0c','',giRaw)\n",
    "giRaw2 = re.sub(r'[a-zA-Z]+(- )[a-zA-Z]+','',giRaw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#대문자로된 약어\n",
    "Aword1 = list(set(re.findall(r' ([A-Z]{2,}) ',giRaw10)))\n",
    "Aword2 = list(set(re.findall(r'\\(([A-Z]{2,})\\)',giRaw10)))\n",
    "Aword10 = Aword1 + Aword2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#알파벳으로 시작하고 숫자로 끝나는 특수용어\n",
    "a9word1 = re.findall(r'\\(([A-Za-z]+[0-9]+)\\)',giRaw10)\n",
    "a9word2 = re.findall(r' ([A-Za-z]+[0-9]+) ',giRaw10)\n",
    "a9word10 = a9word1 + a9word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#보편적인 용어만 가지고 온다.\n",
    "fd = nltk.FreqDist(a9word10)\n",
    "sorted_common_a9word10 = sorted([(c,w) for w, c in fd.items() if c > 4], reverse=1)\n",
    "a9word10_common = [w for c,w in sorted_common_a9word10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그밖의 용어 - 대문자2개 이상 + 소문자유형\n",
    "word3 = re.findall(r' ([A-Z]{2,}[a-z]+) ',giRaw10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그밖의 용어 - 소문자 + 대문자2개 이상 유형\n",
    "word4 = re.findall(r' ([a-z]+[A-Z]{2,}) ',giRaw10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과출력\n",
    "print(\"1. 대문자만으로 된 약어: \",len(set(Aword10)))\n",
    "print(\"2. 알파벳으로 시작하고 숫자로 끝나는 특수 용어: \",len(set(a9word10_common)))\n",
    "print(\"3_1. 대문자로시작해서 소문자로 끝나는 특수 용어 : \",len(set(word3)))\n",
    "print(\"3_2. 소문자로시작해서 대문자로 끝나는 특수 용어 : \",len(set(word4)))\n",
    "result = list(set(Aword10+a9word10_common+word3+word4))\n",
    "print(\"=> 총 용어 수(1+2+3_1+3_2) :\",len(result),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing: 유전자코드 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_genecode = r'[AGCT]{3,}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = [w for w in result if not re.match(re_genecode, w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 태깅을 위한 각 클래스별 약어 Dictionary 생성\n",
    " Biology: https://www.abbreviations.com/acronyms/BIOLOGY\n",
    "  <br> Statistics: https://www.abbreviations.com/acronyms/STATISTICS\n",
    "  <br> Gene: https://www.genenames.org/\n",
    "  <br> protein: https://www.cellsignal.com/contents/resources/protein-and-pathway-abbreviations/pathway-abbreviations\n",
    "  <br><br> 위 링크에서 얻은 소스들로 Biology와 Statistics, gene, protein 4가지 약어 Dictionary를 생성한다.\n",
    "  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 소스들이 있는 path\n",
    "f_root = 'C:/Users/Maeg/AI/Abbreviation'\n",
    "f1_root = 'C:/Users/Maeg/AI/genename'\n",
    "\n",
    "bio_abb_corpus = PlaintextCorpusReader(f_root,'Biology Abbreviations.txt')\n",
    "stat_abb_corpus = PlaintextCorpusReader(f_root,'statistics Abbreviations.txt')\n",
    "genename_corpus = PlaintextCorpusReader(f1_root,'genenames.txt')\n",
    "pro_abb_corpus = PlaintextCorpusReader(f1_root,'proteinname.txt')\n",
    "\n",
    "# 각 소스로부터 약어를 뽑아온다.\n",
    "bio_abb = ('\\r\\n'.join(bio_abb_corpus.raw().split('\\t'))).split('\\r\\n')\n",
    "stat_abb = ('\\r\\n'.join(stat_abb_corpus.raw().split('\\t'))).split('\\r\\n')\n",
    "genenames = genename_corpus.raw().split('\\n')\n",
    "pro_abb_temp = pro_abb_corpus.raw().split('\\r\\n')\n",
    "pro_abb = []\n",
    "for i in range(len(pro_abb_temp)):\n",
    "    s = re.findall(r'([A-Za-z-0-9]+)\\t', pro_abb_temp[i])\n",
    "    pro_abb.append(s[0])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 해당 약어가 포함된 클래스를 리턴, 어떤 클래스에도 속하지 않을경우 0을 리턴한다.\n",
    "def Check_class(str):\n",
    "    count = 0\n",
    "    classes = []\n",
    "    \n",
    "    if str in bio_abb:\n",
    "        count += 1\n",
    "        classes.append('BIO')\n",
    "    if str in stat_abb:\n",
    "        count += 1\n",
    "        classes.append('STAT')\n",
    "    if str in pro_abb:\n",
    "        count += 1\n",
    "        classes.append('PRO')\n",
    "    if str in genenames:\n",
    "        count += 1\n",
    "        classes.append('GENE')\n",
    "    \n",
    "    if count >= 1:\n",
    "        return classes\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문맥을 고려해 클래스 지정함수 만들기\n",
    "https://en.wikipedia.org/wiki/Biology <br>\n",
    "https://en.wikipedia.org/wiki/Gene <br>\n",
    "https://en.wikipedia.org/wiki/Glossary_of_biology<br>\n",
    "https://en.wikipedia.org/wiki/Glossary_of_probability_and_statistics<br>\n",
    "https://en.wikipedia.org/wiki/Protein<br>\n",
    "https://en.wikipedia.org/wiki/Statistics<br>\n",
    "\n",
    "약어 주변에 어떤 단어들이 오는지 확인하고 4가지 클래스와 연관성을 도출하기 위해서 Word2Vec모델을 만든다.<br>\n",
    "Word2Vec모델을 만드는데에 위 링크의 글들을 활용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path= 'C:/Users/Maeg/AI/data'\n",
    "corpus = PlaintextCorpusReader(path,\".*\\.txt\")\n",
    "\n",
    "#학습 문장 생성\n",
    "sentences = corpus.sents()\n",
    "\n",
    "model = Word2Vec(sentences, size=300, window=3, min_count=1, workers=1)\n",
    "word_vectors = model.wv\n",
    "vocabs = list(word_vectors.vocab.keys())\n",
    "classes = ['biology', 'gene', 'protein', 'statistical']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# return tag of abbreviation. with around wording.\n",
    "def TOA_GNI(str, classes):\n",
    "    if str in giword_list :\n",
    "        # 해당 단어가 GNI 코퍼스 상에서 위치하는 곳\n",
    "        idx = giword_list.index(str)\n",
    "        concordance = giword_list[idx-3:idx+4] # 주변단어들 : 앞뒤로 3개의 단어들.\n",
    "        \n",
    "        # 이전 Check_Class에서의 속한 클래스가 있었다면 해당 클래스에 대해서만 연관성 분석\n",
    "        if classes == 0 :\n",
    "            cla = ['biology', 'gene', 'protein', 'statistical']\n",
    "        else :\n",
    "            cla = classes\n",
    "        \n",
    "        max_class = ''\n",
    "        max_sim = -1\n",
    "        \n",
    "        # 최고 연관도(주변단어들과 class의 연관도의 평균)를 구한다.\n",
    "        for c in cla :\n",
    "            count = 0;\n",
    "            sim = 0;\n",
    "            for w in concordance :\n",
    "                # 주변단어일 경우, 모델에 해당 단어가 있는지 확인하고 연관도를 출력한다.\n",
    "                if w != str :\n",
    "                    if w in vocabs :\n",
    "                        sim += word_vectors.similarity(c, w)\n",
    "                        count += 1\n",
    "            sim /= count # 연관도의 평균\n",
    "            if max_sim < sim :\n",
    "                max_sim = sim\n",
    "                max_class = c\n",
    "        \n",
    "        # 리턴한다.\n",
    "        if max_sim != -1 :\n",
    "            return max_class\n",
    "        # 값이 유효하지 않을경우 태깅 불가\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    #str을 참조할수 없는 경우 태깅 불가\n",
    "    else :\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 약어 태깅하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Result)):\n",
    "    # 약어를 태깅한다.\n",
    "    cla = Check_class(Result[i])\n",
    "    if cla != 0 :\n",
    "        print(Result[i], Check_class(Result[i])) \n",
    "    else:\n",
    "        print(Result[i], TOA_GNI(str, cla))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-c329c30d781c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgiword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36m__len__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    397\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pieces\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m             \u001b[1;31m# Iterate to the end of the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 399\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_offsets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    400\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m             \u001b[1;31m# Get everything we can from this piece.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpiece\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterate_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_tok\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0moffset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mtok\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\util.py\u001b[0m in \u001b[0;36miterate_from\u001b[1;34m(self, start_tok)\u001b[0m\n\u001b[0;32m    310\u001b[0m             )\n\u001b[0;32m    311\u001b[0m             \u001b[0mnum_toks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[0mnew_filepos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             assert new_filepos > filepos, (\n\u001b[0;32m    314\u001b[0m                 \u001b[1;34m'block reader %s() should consume at least 1 byte (filepos=%d)'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mtell\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1422\u001b[0m             \u001b[0mcheck1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_incr_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[0mcheck2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinebuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1424\u001b[1;33m             \u001b[1;32massert\u001b[0m \u001b[0mcheck1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcheck2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1426\u001b[0m         \u001b[1;31m# Return to our original filepos (so we don't have to throw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15689"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "giRaw10.index('AA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 0.9999667406082153),\n",
       " ('(', 0.9999650120735168),\n",
       " ('that', 0.9999643564224243),\n",
       " ('and', 0.9999639987945557),\n",
       " ('for', 0.9999638795852661),\n",
       " (',', 0.9999637603759766),\n",
       " ('in', 0.9999631643295288),\n",
       " ('or', 0.9999629855155945),\n",
       " ('.', 0.999962568283081),\n",
       " ('the', 0.9999625086784363)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
